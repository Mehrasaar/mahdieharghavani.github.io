---
layout: persian  # یا single با کلاس rtl-layout
classes: wide rtl-layout
dir: rtl
title: "امتحان میان‌ترم - فرآیندهای گوسی"
permalink: /teaching/toolkit/GP/
author_profile: false
sidebar:
  nav: "toolkit"
header:
  overlay_image: "/assets/images/background.jpg"
  overlay_filter: 0.3
  overlay_color: "#5e616c"
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
---

# امتحان میان‌ترم - فرآیندهای گوسی

## دوره: یادگیری ماشین

## ترم: پاییز 1403

## **نویسندگان**
این سند توسط موارد زیر تهیه شده است:
- Mahdieh Alizadeh , Mehrnoosh Ziaei, Behzad Sabeti, Payam Parvazmanesh, Poorya Fekri, Erfan fakoor



## 1. **مدل‌سازی مسئله**

ما رابطه بین خروجی‌های مشاهده‌شده $ ( y_i ) $ و بردارهای ورودی $ ( x_i ) $ را با یک تابع خطی مدل می‌کنیم:

$$
y_i = w^\top x_i + \epsilon_i,
$$
که در آن:
- $ ( y_i ) $ خروجی مشاهده‌شده برای $ i $-مین نقطه داده است.
- $ ( x_i \in \mathbb{R}^d ) $ بردار ورودی برای $ i $-مین نقطه داده است، که در آن $ d $ تعداد ویژگی‌ها است.
- $ ( w \in \mathbb{R}^d ) $ بردار وزن (پارامترهای ناشناخته) است.
- $ ( \epsilon_i \sim \mathcal{N}(0, \sigma^2) ) $ نویز گوسی با میانگین 0 و واریانس $ ( \sigma^2 ) $ است.

دستگاه داده شامل $ n $ مشاهده مستقل $ \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} $ است.



## 2. **توزیع پیشین روی وزن‌ها**

ما یک **توزیع پیشین گوسی** برای بردار وزن $ ( w ) $ فرض می‌کنیم، که باور ما در مورد وزن‌ها قبل از دیدن هرگونه داده را نشان می‌دهد. توزیع پیشین به صورت زیر است:

$$
p(w) = \mathcal{N}(0, \Sigma_p),
$$
که در آن:
- $ ( \Sigma_w \in \mathbb{R}^{d \times d} ) $ ماتریس کوواریانس توزیع پیشین روی $ ( w ) $ است. این می‌تواند بر اساس دانش پیشین در مورد وزن‌ها انتخاب شود یا به صورت $ ( \Sigma_p = \lambda I ) $ تنظیم شود، که در آن $ ( I ) $ ماتریس همانی و $ ( \lambda > 0 ) $ یک پارامتر تنظیم‌گر است.



## 3. **تابع درستنمایی**

درستنمایی داده‌های مشاهده‌شده $ ( y = [y_1, y_2, \dots, y_n]^\top ) $ با توجه به ورودی‌ها $ ( X = [x_1^\top, x_2^\top, \dots, x_n^\top]^\top ) $ و وزن‌ها $ ( w ) $ به صورت زیر است:

$$
p(y|X,w) = \prod_{i=1}^n p(y_i|x_i, w) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2_n}} \exp\left( -\frac{(y_i - w^\top x_i)^2}{2\sigma^2_n} \right)
$$

$$
= \frac{1}{(2\pi\sigma^2_n)^{n/2}} \exp\left( -\frac{1}{2\sigma^2_n} \|y - w^\top X\|^2 \right)
$$

$$
p(y|X,w)=\mathcal{N}(w^\top X, \sigma^2)
$$

که در آن:
- $ y $ بردار مقادیر هدف مشاهده‌شده است.
- $ X $ ماتریس بردارهای ورودی است.
- $ \sigma^2_n $ واریانس نویز است.
- درستنمایی $ p(y|X,w) $ احتمال مشاهده $ y $ با توجه به $ X $ و $ w $ را نشان می‌دهد.



## 4. **توزیع پسین وزن‌ها** (**مرحله آموزش**)

با استفاده از **قاعده بیز**، توزیع پسین $ ( w ) $ با توجه به داده‌ها $ ( X ) $ و $ ( y ) $ به صورت زیر است:

$$
p(w|y,X) = \frac{p(y|X,w) p(w)}{p(y|X)}
$$

همانطور که می‌خواهیم $p(w|y,X)$ را محاسبه کنیم، می‌توانیم فرض کنیم $p(y|X)$ یک ثابت است و این عضو در تناسب جذب می‌شود.
$$
p(w|y,X) \propto p(y|X,w)*p(w)
$$

همانطور که از توزیع پیشین می‌دانیم $w \sim \mathcal{N}(0, \Sigma_p)$، بنابراین $p(w|y,X) \propto \exp\left( -\frac{1}{2} w^\top \Sigma_p^{-1} w \right) $

و همچنین داریم $y = x^\top w + \epsilon$. بنابراین وقتی $X,w$ را می‌دانیم، $y$ مانند $ \epsilon $ رفتار می‌کند که با توجه به $x^\top w$ تغییر می‌کند. این یعنی $y \sim \mathcal{N}(X^\top w, \sigma^2)$ بنابراین، $p(y|X,w) \propto \exp\left( -\frac{1}{2\sigma^2_n} (y - X^\top w)^\top (y - X^\top w) \right)$

بنابراین:

$$
p(w|X,y) \propto \exp\left( -\frac{1}{2\sigma^2_n} (y - w^\top X)^\top (y - w^\top X) \right) \exp\left( -\frac{1}{2} w^\top \Sigma_p^{-1} w \right)
$$

### مراحل استخراج توزیع پسین

ما با عبارت زیر برای توزیع پسین $ w $ با توجه به داده‌ها $ X $ و $ y $ شروع می‌کنیم:

$$
p(w \mid X, y) \propto \exp\left( -\frac{1}{2\sigma^2_n} (y - w^\top X)^\top (y - w^\top X) \right) \exp\left( -\frac{1}{2} w^\top \Sigma_p^{-1} w \right)
$$

### **مرحله 0: محصول دو توزیع گوسی چیست؟**
در لینک زیر می‌توانیم پاسخ را ببینیم:

[Ploting the product of two Gaussian distributions](https://www.desmos.com/calculator/qtua9ikyhc)

جالب است ببینیم که میانگین و کوواریانس جدید چگونه محاسبه می‌شوند. میانگین جدید در واقع میانگین وزن‌دار میانگین‌های قدیمی با معکوس ماتریس‌های کوواریانس است. فرض کنید توزیع $X_1$ باریک باشد و واریانس پایینی در مقایسه با توزیع $X_2$ داشته باشد. توزیع دوم گوسی در اینجا واریانس بسیار بزرگی دارد، بنابراین یک توزیع بسیار گسترده و پهناور است. در حالی که $X_1$ بسیار باریک است و کوواریانس بسیار کوچکی دارد، بنابراین متغیر تصادفی $X_1$ ناآرامی بسیار کمتری نسبت به متغیر تصادفی $X_2$ دارد. در نتیجه، نتیجه اشتراک دو توزیع به شدت تحت تأثیر $ \mu_1 $ خواهد بود، یعنی متغیر تصادفی اول، زیرا $ \Sigma_2 $ بزرگ و $ \Sigma_1 $ نسبتاً کوچک است. به همین ترتیب، تأثیر $ \mu_2 $ محدود خواهد بود.

کوواریانس جدید در واقع معکوس مجموع معکوس‌های کوواریانس فردی است. واریانس جدید $ \sigma_2 $ دو برابر میانگین هارمونیک واریانس‌های فردی است.

به طور خلاصه، این یعنی PDFهای دو توزیع گوسی g1(x) و g2(x) محصولی گوسی مقیاس‌شده است. همچنین ناآرامی کاهش می‌یابد، بنابراین ماتریس کوواریانس کوچک‌تر می‌شود، بنابراین اگر دو توزیع گوسی را ترکیب کنید، نتیجه باریک‌تر و قطعیت بیشتری نسبت به کوواریانس‌های اصلی خواهد داشت. و میانگین توزیع جدید متناسباً نزدیک‌تر به توزیع باریک‌تر خواهد بود.

### **مرحله 1: ترکیب اکسپونانسیل‌ها**

ما دو عضو اکسپونانسیل از درستنمایی و پیشین را ترکیب می‌کنیم:

$$
p(w \mid X, y) \propto \exp\left( -\frac{1}{2\sigma^2_n} (y - w^\top X)^\top (y - w^\top X) - \frac{1}{2} w^\top \Sigma_p^{-1} w \right)
$$

حالا ترم‌های درجه دوم داخل نماد توان را گسترش می‌دهیم.



### **مرحله 2: گسترش ترم‌های درجه دوم**

#### ترم اول: $ ( (y - w^\top X)^\top (y - w^\top X) ) $

ما ترم درجه دوم اول را به صورت زیر گسترش می‌دهیم:

$$
(y - w^\top X)^\top (y - w^\top X) = y^\top y - 2 y^\top w^\top X + w^\top X^\top X w
$$

بنابراین، ترم اول در نماد توان به صورت زیر در می‌آید:

$$
-\frac{1}{2\sigma^2_n} \left( y^\top y - 2 y^\top w^\top X + w^\top X^\top X w \right)
$$

#### ترم دوم: $ ( -\frac{1}{2} w^\top \Sigma_p^{-1} w ) $

این ترم بدون تغییر باقی می‌ماند:

$$
-\frac{1}{2} w^\top \Sigma_p^{-1} w
$$



### **مرحله 3: ترکیب تمام ترم‌ها**

حالا، ترم‌های گسترش‌یافته را ترکیب می‌کنیم:

$$
p(w \mid X, y) \propto \exp\left( -\frac{1}{2\sigma^2_n} y^\top y + \frac{1}{\sigma^2_n} y^\top w^\top X - \frac{1}{2\sigma^2_n} w^\top X^\top X w - \frac{1}{2} w^\top \Sigma_p^{-1} w \right)
$$

از آنجایی که $ ( y^\top y ) $ مستقل از $ ( w ) $ است، می‌توانیم این عضو ثابت را نادیده بگیریم، و عبارت به صورت زیر ساده می‌شود:

$$
p(w \mid X, y) \propto \exp\left( \frac{1}{\sigma^2_n} y^\top w^\top X - \frac{1}{2\sigma^2_n} w^\top X^\top X w - \frac{1}{2} w^\top \Sigma_p^{-1} w \right)
$$



### **مرحله 4: تکمیل مربع**

با توجه به عبارت:

$$
p(w \mid X, y) \propto \exp\left( \frac{1}{\sigma^2_n} y^\top w^\top X - \frac{1}{2\sigma^2_n} w^\top X^\top X w - \frac{1}{2} w^\top \Sigma_p^{-1} w \right)
$$

این عبارت در $ (w) $ درجه دوم است و یک توزیع گوسی را نشان می‌دهد. بنابراین:
1. **مشتق اول** نسبت به $ ( w ) $ نقطه بحرانی را می‌دهد که معادل میانگین (برآورد MAP) توزیع گوسی است.
2. **مشتق دوم** (هسیان) انحنای را تعیین می‌کند، و معکوس منفی آن ماتریس کوواریانس توزیع پسین را فراهم می‌کند.

#### مشتق اول (گرادیان)

فرض کنید $ (f(w)) $ نماد توان توزیع پسین باشد:

$$
f(w) = \frac{1}{\sigma^2_n} y^\top w^\top X - \frac{1}{2\sigma^2_n} w^\top X^\top X w - \frac{1}{2} w^\top \Sigma_p^{-1} w
$$

گرادیان $ ( 
abla_w f(w)) $ به صورت زیر است:

$$

abla_w f(w) = \frac{\partial}{\partial w} \left( \frac{1}{\sigma^2_n} y^\top w^\top X - \frac{1}{2\sigma^2_n} w^\top X^\top X w - \frac{1}{2} w^\top \Sigma_p^{-1} w \right)
$$

تفکیک هر ترم:

1. برای ترم اول:

$$
\frac{\partial}{\partial w} \left( \frac{1}{\sigma^2_n} y^\top w^\top X \right) = \frac{1}{\sigma^2_n} X^\top y
$$

2. برای ترم دوم:

$$
\frac{\partial}{\partial w} \left( -\frac{1}{2\sigma^2_n} w^\top X^\top X w \right) = -\frac{1}{\sigma^2_n} X^\top X w
$$

3. برای ترم سوم:

$$
\frac{\partial}{\partial w} \left( -\frac{1}{2} w^\top \Sigma_p^{-1} w \right) = -\Sigma_p^{-1} w
$$

ترکیب این نتایج:

$$

abla_w f(w) = \frac{1}{\sigma^2_n} X^\top y - \frac{1}{\sigma^2_n} X^\top X w - \Sigma_p^{-1} w
$$

قرار دادن گرادیان برابر صفر برای یافتن نقطه بحرانی $ ( \bar{w}) $:

$$
\frac{1}{\sigma^2_n} X^\top y - \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right) \bar{w} = 0
$$

حل $ (\bar{w}) $:

$$
\bar{w} = \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1} \frac{1}{\sigma^2_n} X^\top y
$$

#### مشتق دوم (هسیان)

ماتریس هسیان $ (H) $ مشتق دوم $ (f(w)) $ نسبت به $ (w) $ است:

$$

abla_w^2 f(w) = \frac{\partial}{\partial w} \left( -\frac{1}{\sigma^2_n} X^\top X w - \Sigma_p^{-1} w \right)
$$

بنابراین:

$$
H = -\left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)
$$

سپس همانطور که قبلاً گفتیم:
$A=-(H^{-1})$

### **مرحله 5: توزیع پسین نهایی**

بنابراین، توزیع پسین $ ( w ) $ به صورت زیر است:

$$
p(w \mid X, y) = \mathcal{N}\left( \bar{w}, \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1} \right)
$$

که در آن:
- **میانگین پسین** $ ( \bar{w} ) $ به صورت زیر است:

$$
\bar{w} = \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1} \frac{1}{\sigma^2_n} X^\top y
$$

- **کوواریانس پسین** $ ( A^{-1} ) $ به صورت زیر است:

$$
A^{-1} = \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1}
$$



### **خلاصه فرمول‌های کلیدی:**

- **توزیع پسین** یک توزیع گوسی است:

$$
p(w \mid X, y) = \mathcal{N}\left( \bar{w}, A^{-1} \right)
$$

- **میانگین پسین** $ ( \bar{w} ) $ به صورت زیر است:

$$
\bar{w} = \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1} \frac{1}{\sigma^2_n} X^\top y
$$

- **کوواریانس پسین** $ ( A^{-1} ) $ به صورت زیر است:

$$
A^{-1} = \left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1}
$$

- این میانگین و واریانس چه معنایی دارند؟

1. آنچه در فرمول میانگین می‌بینیم این است که میانگین توزیع پسین به سمت میانگین دانش پیشین ما تمایل دارد اگر ناآرامی داده‌های آموزشی بالا باشد یا تعداد داده‌ها کم باشد.
2. از سوی دیگر، اگر ناآرامی داده‌های آموزشی پایین باشد یا تعداد داده‌ها زیاد باشد، میانگین توزیع پسین به سمت میانگین داده‌های آموزشی تمایل دارد.
3. اگر به دقت نگاه کنیم، $y$ در فرمول میانگین ظاهر شده اما در کوواریانس ظاهر نشده است. این یعنی کوواریانس و ناآرامی که تخمین می‌زنیم مستقل از داده‌های مشاهده‌شده است و این یکی از معایب فرآیندهای گوسی است.
4. کوواریانس توزیع پسین فقط به دانش پیشین و داده‌های آموزشی بستگی دارد.
5. همانطور که در فرمول کوواریانس می‌بینیم، اگر ناآرامی داده‌های آموزشی بالا باشد یا تعداد داده‌های آموزشی کم باشد، توزیع پسین ناآرامی خود را بر اساس ناآرامی دانش پیشین تعیین می‌کند.
6. از سوی دیگر، اگر حجم داده‌های آموزشی به اندازه کافی زیاد باشد یا ناآرامی داده‌های آموزشی پایین باشد؛ توزیع پسین ناآرامی خود را بر اساس ناآرامی داده‌های آموزشی تعیین می‌کند.

این نتیجه‌گیری استخراج توزیع پسین $ ( p(w \mid X, y) ) $ با نماد $ ( w^\top X ) $ است.

## 5. **توزیع پیش‌بینی $ ( f_* ) $**


برای پیش‌بینی برای یک مورد آزمون، میانگین روی تمام مقادیر ممکن توزیع پیش‌بینی پارامتر را با وزن‌دهی متناسب با احتمال پسین آن‌ها میانگین‌گیری می‌کنیم. این در تضاد با رویکردهای غیربیز است، که در آن معمولاً یک پارامتر با یک معیار انتخاب می‌شود. بنابراین، توزیع پیش‌بینی برای $ ( f_* ) $، $ ( f(x_*) ) $ در $ ( x_* ) $ با میانگین‌گیری خروجی تمام مدل‌های خطی ممکن با توجه به توزیع گوسی پسین به صورت زیر داده می‌شود:

$$
p(f_* \mid x_*, X, y) = \int p(f_* \mid x_*, w) p(w \mid X, y) \, dw.
$$



از آنجایی که $ ( p(y_* \mid x_*, w) = \mathcal{N}( w^\top x_*, \sigma^2) ) $، توزیع پیش‌بینی نیز گوسی است. میانگین و واریانس توزیع پیش‌بینی را می‌توان به صورت زیر محاسبه کرد.

###  **میانگین پیش‌بینی $ ( f_* )$**:

$$
\mathbb{E}[f_* \mid X,y,x_*] 
$$



$$
\mathbb{E}[w^\top x_* \mid X,y,x_*] 
$$



$$
\mathbb{E}[x_*^\top w \mid X,y,x_*] 
$$

$$
x_*^\top \mathbb{E}[ w \mid X,y]
$$

$$
x_*^\top \bar{w}
$$


$$
\text{mean}(f_*) = x_*^\top(\left( \frac{1}{\sigma^2_n} X^\top X + \Sigma_p^{-1} \right)^{-1} \frac{1}{\sigma^2_n} X^\top y)
$$

###  **واریانس پیش‌بینی $ ( f_* )$**



$$
\text{Var}(f_*) = \mathbb{E}[ (f_* - \mathbb{E}[f_* \mid X,y,x_*])^2 \mid X,y,x_* ]
$$


$$
\mathbb{E}[ (w^\top x_* - \bar{w}^\top x_*)^2 \mid X,y,x_*]
$$

$$
\mathbb{E}[((w- \bar{w})^\top x_*)^2 \mid X,y,x_*]
$$

$$
\mathbb{E}[x_*^\top (w-\bar{w})(w-\bar{w})^\top x_* \mid X,y,x_*]
$$

$$
x_*^\top \mathbb{E}[ (w-\bar{w})(w-\bar{w})^\top \mid X,y] x_*
$$

$$
\text{Var}(f_*) = x_*^\top A^{-1} x_*
$$




## 6. **توزیع پیش‌بینی نهایی**

بنابراین، توزیع پیش‌بینی برای $ ( f_* ) $ به صورت زیر است:

$$
f_* \mid x_*, X, y \sim \mathcal{N}(\mu_*, \sigma_*^2),
$$
که در آن:
- **میانگین پیش‌بینی** به صورت زیر است:

$$
\mu_* = x_*^\top A^{-1} \left( \frac{1}{\sigma^2} X^\top y \right),
$$

- **واریانس پیش‌بینی** به صورت زیر است:

$$
\sigma_*^2 = x_*^\top A^{-1} x_* 
$$

## خلاصه فرمول‌های کلیدی:

- **پیشین**: $ p(w) = \mathcal{N}(0, \Sigma_p) $
- **درستنمایی**: $ p(y \mid X, w) = \mathcal{N}( w^\top X, \sigma^2) $
- **پسین**: $ p(w \mid X, y) = \mathcal{N}(\bar{w}, A^{-1}) $
 

 - $ \bar{w} = A^{-1} \left( \frac{1}{\sigma^2} X^\top y \right) $
  - $ A^{-1} = \left( \Sigma_p^{-1} + \frac{1}{\sigma^2} X^\top X \right)^{-1} $
- **میانگین پیش‌بینی**: $\mu_* = x_*^\top A^{-1} \left( \frac{1}{\sigma^2} X^\top y \right)$
- **واریانس پیش‌بینی**: $\sigma_*^2 = x_*^\top A^{-1} x_*$


# مدیریت ابعاد بالا و هسته‌ها
با توجه به فرمول توزیع پسین مقادیر تابع $ f_* $ در یک نقطه آزمون $ x_* $:

$$
f_* | x_*, X, y \sim \mathcal{N}\left( \frac{1}{\sigma^2_n} \phi(x_*)^\top A^{-1} \phi^\top y, \phi(x_*)^\top A^{-1} \phi(x_*) \right)
$$

که در آن:
- $ \phi(x_*) $ نگاشت ویژگی نقطه آزمون $ x_* $ است،
- $ \Phi = [\phi(x_1), \phi(x_2), ..., \phi(x_N)] $ ماتریس نگاشت‌های ویژگی برای داده‌های آموزشی است،
- $ A = \sigma_n^{-2} \Phi \Phi^\top + \Sigma_p^{-1} $ ماتریس دقت است، که نویز دقت و دقت پیشین را ترکیب می‌کند.

ما می‌خواهیم این را به صورت **هسته‌ها** بیان کنیم، زیرا دلایل متعددی وجود دارد:
هسته‌ها در زمینه‌های مختلف استفاده می‌شوند، به ویژه زمانی که با مدل‌هایی کار می‌کنیم که نیاز به عملکرد در فضاهای ورودی با ابعاد بالا یا پیچیده دارند. در اینجا دلایل استفاده از هسته‌ها آورده شده است:

## چه زمانی از هسته‌ها در فرآیندهای گوسی استفاده کنیم؟
هسته‌ها (که همچنین به عنوان توابع کوواریانس شناخته می‌شوند) نقش حیاتی در فرآیندهای گوسی (GP) ایفا می‌کنند، که مدل‌های غیرپارامتری هستند که برای وظایف رگرسیون و طبقه‌بندی استفاده می‌شوند. در GP، هسته‌ها ساختار کوواریانس تابع زیرین را که سعی در مدل‌سازی آن دارید، تعریف می‌کنند. در اینجا چرا و چه زمانی از هسته‌ها در فرآیندهای گوسی استفاده می‌شود:

### 1. **برای تعریف کوواریانس بین نقاط داده**
- هسته کوواریانس بین هر دو نقطه در فضای ورودی را تعریف می‌کند. این برای فرآیندهای گوسی حیاتی است زیرا GPs فرض می‌کنند که هر مجموعه‌ای از مقادیر تابع یک توزیع گوسی مشترک دارد و ماتریس کوواریانس توسط هسته تعریف می‌شود.
- **مثال**: اگر نقاط ورودی $ X $ داشته باشید و بخواهید مقدار تابع در یک نقطه جدید $ x_* $ را پیش‌بینی کنید، هسته مشخص می‌کند که $ x_* $ چقدر به نقاط آموزشی $ X $ شباهت یا همبستگی دارد.

### 2. **برای کدگذاری فرضیات در مورد تابعی که مدل می‌شود**
- هسته‌ها به شما اجازه می‌دهند فرضیات در مورد ویژگی‌های تابع زیرین را بیان کنید، مانند نرمی، تناوب یا سطح نویز.
  - **نرمی**: می‌توان فرض کرد که تابع پیوسته و مشتق‌پذیر باشد (مثلاً با استفاده از هسته RBF).
  - **تناوب**: می‌توان فرض کرد که تابع خود را در طول زمان تکرار می‌کند (مثلاً با استفاده از هسته تناوبی).
  - **نویز**: هسته‌ها همچنین می‌توانند یک عضو نویز را برای مدل‌سازی نویز مشاهده در داده‌ها اضافه کنند (مثلاً با افزودن یک هسته نویز).
- **مثال**: در رگرسیون GP، انتخاب هسته کنترل نرمی تابع پیش‌بینی شده و اینکه مدل چقدر به نقاط داده نزدیک وابسته است.

### 3. **برای مدل‌سازی روابط غیرخطی**
- هسته‌ها زمانی مفید هستند که رابطه بین متغیرهای ورودی و خروجی غیرخطی باشد. با استفاده از یک تابع هسته، مدل GP می‌تواند داده‌ها را به صورت ضمنی به فضای با ابعاد بالاتر نگاشت کند، که به آن اجازه می‌دهد روابط پیچیده و غیرخطی را بدون تبدیل صریح داده‌ها مدل‌سازی کند.
- **مثال**: هسته تابع پایه شعاعی (RBF) که همچنین به عنوان هسته گوسی شناخته می‌شود، برای مدل‌سازی روابط صاف و غیرخطی به طور گسترده‌ای استفاده می‌شود. این به GP اجازه می‌دهد پیش‌بینی‌های دقیقی برای توابع غیرخطی، مانند آن‌هایی که در سری‌های زمانی یا داده‌های فضایی یافت می‌شوند، انجام دهد.

### 4. **برای دربرگرفتن دانش پیشین در مورد داده‌ها**
- در GP، تابع هسته راهی برای دربرگرفتن دانش پیشین یا تخصص دامنه در مورد تابعی که مدل می‌کنید، فراهم می‌کند. برای مثال، اگر بدانید که تابع رفتار تناوبی دارد، می‌توانید از یک هسته تناوبی استفاده کنید.
- **مثال**: اگر در حال مدل‌سازی دمای در طول زمان هستید، یک هسته تناوبی فرضیه را که دما با یک دوره خاص تکرار می‌شود، کدگذاری می‌کند.

### 5. **برای بیان ناآرامی در پیش‌بینی‌ها**
- فرآیندهای گوسی مدل‌های احتمالاتی هستند و هسته نه تنها به مدل کمک می‌کند تا پیش‌بینی‌ها را انجام دهد، بلکه ناآرامی در پیش‌بینی‌ها را نیز کمی‌سازی می‌کند. این به ویژه در تنظیمات مفید است که در آن نیاز به درک اطمینان در خروجی مدل دارید، مانند در یادگیری فعال یا وظایف بهینه‌سازی.

### 6. **برای مدیریت فضاهای ورودی با ابعاد بالا یا پیچیده**
- بسیاری از مسائل، به ویژه در یادگیری ماشین و آمار، شامل داده‌های با ابعاد بالا یا پیچیده هستند. هسته‌ها راهی برای محاسبه ضرب داخلی در فضاهای ویژگی با ابعاد بالا فراهم می‌کنند (با استفاده از "ترفند هسته")، که به فرآیندهای گوسی اجازه می‌دهد داده‌ها را در این فضاها مدل‌سازی کنند بدون نیاز به محاسبه صریح نگاشت‌های با ابعاد بالاتر.

### 7. **برای مدل‌سازی ساختارهای پیچیده داده**
- هسته‌ها می‌توانند برای مدیریت انواع داده‌های پیچیده مانند سری‌های زمانی، تصاویر یا متن استفاده شوند. با استفاده از هسته‌های تخصصی، GPs می‌توانند روابطی را که با مدل‌های خطی استاندارد دشوار است، مدل‌سازی کنند.
- **مثال**: در پیش‌بینی سری‌های زمانی، یک هسته که همبستگی‌های زمانی را در نظر می‌گیرد (مانند هسته RBF زمانی) می‌تواند برای مدل‌سازی نحوه وابستگی مقادیر آینده به مقادیر گذشته در یک فرآیند پیوسته استفاده شود.

### 8. **برای دستیابی به انعطاف‌پذیری غیرپارامتری**
- فرآیندهای گوسی غیرپارامتری هستند، یعنی آن‌ها فرم مشخصی برای تابعی که مدل می‌شود، فرض نمی‌کنند. در عوض، تابع هسته فضا را از توابع ممکن تعریف می‌کند. این ماهیت غیرپارامتری به ویژه زمانی مفید است که دانش کمی در مورد شکل تابع beforehand دارید.

### خلاصه چرا هسته‌ها در فرآیندهای گوسی ضروری هستند
- **مدل‌سازی کوواریانس**: هسته‌ها نحوه ارتباط نقاط داده از طریق کوواریانس را تعریف می‌کنند، که برای GP ضروری است.
- **کدگذاری فرضیات**: هسته‌ها به شما اجازه می‌دهند فرضیات در مورد نرمی، تناوب و سایر ویژگی‌های تابع را بیان کنید.
- **غیرخطی**: هسته‌ها به GPs اجازه می‌دهند تا روابط پیچیده و غیرخطی بین ورودی‌ها و خروجی‌ها را مدل‌سازی کنند.
- **دانش پیشین**: هسته‌ها راهی برای دربرگرفتن دانش دامنه در مورد داده‌ها فراهم می‌کنند.
- **کمی‌سازی ناآرامی**: در GPs، هسته‌ها به مدل‌سازی ناآرامی در پیش‌بینی‌ها کمک می‌کنند.
- **داده‌های با ابعاد بالا**: هسته‌ها به GPs اجازه می‌دهند به طور کارآمد با فضاهای ورودی با ابعاد بالا کار کنند.
- **ساختارهای پیچیده داده**: هسته‌های تخصصی به GPs اجازه می‌دهند تا ساختارهای پیچیده داده، مانند سری‌های زمانی و داده‌های فضایی را مدیریت کنند.
- **انعطاف‌پذیری غیرپارامتری**: GPs با هسته‌ها یک رویکرد انعطاف‌پذیر و غیرپارامتری برای مدل‌سازی داده‌ها ارائه می‌دهند.

### هسته‌های رایج استفاده‌شده در فرآیندهای گوسی
- **هسته خطی**: مناسب برای مدل‌سازی روابط خطی.
- **هسته تابع پایه شعاعی (RBF) / هسته گوسی**: توابع صاف و پیوسته را در بر می‌گیرد، به طور گسترده در رگرسیون GP استفاده می‌شود.
- **هسته ماترن**: انعطاف‌پذیری بیشتری نسبت به هسته RBF دارد، به گونه‌ای که اجازه می‌دهد درجات مختلف نرمی را داشته باشد.
- **هسته تناوبی**: برای مدل‌سازی توابع تناوبی، مانند در سری‌های زمانی با الگوی تکراری، مفید است.
- **هسته نویز**: یک عضو نویز را برای مدل‌سازی نویز مشاهده در داده‌ها اضافه می‌کند.

### مثال از هسته‌های استفاده‌شده در فرآیندهای گوسی

مثال رایج استفاده‌شده در فرآیندهای گوسی، **هسته تابع پایه شعاعی (RBF)** است، که همچنین به عنوان **هسته گوسی** شناخته می‌شود، که به صورت زیر تعریف می‌شود:

$$
k(x, x') = \exp\left(-\frac{|x - x'|^2}{2 \sigma^2}\right)
$$

که در آن $ \sigma $ یک هایپرپارامتر است که نرمی تابعی که مدل می‌شود را کنترل می‌کند.

دیگری که به طور گسترده استفاده می‌شود، **هسته ماترن** است، که به صورت زیر تعریف می‌شود:

$$
k(x, x') = \frac{1}{\Gamma(
u) 2^{
u - 1}} \left(\frac{\sqrt{2 
u} |x - x'|}{\ell}\right)^
u K_
u\left(\frac{\sqrt{2 
u} |x - x'|}{\ell}\right)
$$

که در آن $ 
u $ نرمی را کنترل می‌کند و $ \ell $ پارامتر مقیاس طول است.

**هسته‌ها** در فرآیندهای گوسی انعطاف‌پذیری و قدرت زیادی را فراهم می‌کنند، که به مدل‌ها اجازه می‌دهد تا روابط پیچیده و غیرخطی در داده‌ها را در بر بگیرند در حالی که ناآرامی در پیش‌بینی‌ها را کمی‌سازی می‌کنند.

## استفاده از ترفند هسته
برای یک GP، کوواریانس بین نقاط آموزشی $ \mathbf{X} $ و نقطه آزمون $ \mathbf{x_*} $ توسط تابع هسته تعیین می‌شود:

1. **ماتریس کوواریانس بین نقاط آموزشی**:
$$
K(\mathbf{X}, \mathbf{X}) = \Phi(\mathbf{X}) \Phi(\mathbf{X})^\top
$$
که در آن $ \Phi(\mathbf{X}) $ نگاشت ویژگی مرتبط با نقاط داده در $ \mathbf{X} $ است.

2. **کوواریانس بین نقاط آموزشی و آزمون**:
$$
K(\mathbf{X}, \mathbf{x_*}) = \Phi(\mathbf{X}) \Phi(\mathbf{x_*})^\top
$$
که در آن $ \Phi(\mathbf{x_*}) $ نگاشت ویژگی برای نقطه آزمون $ \mathbf{x_*} $ است.

3. **ماتریس کوواریانس نقاط آزمون**:
$$
K(\mathbf{x_*}, \mathbf{x_*}) = \Phi(\mathbf{x_*}) \Phi(\mathbf{x_*})^\top
$$

با استفاده از ترفند هسته، نیازی به محاسبه صریح $ \Phi(\mathbf{X}) $ یا $ \Phi(\mathbf{x_*}) $ نیست. در عوض، مقادیر هسته را مستقیماً محاسبه می‌کنید، که به مدل GP اجازه می‌دهد بر اساس این مقادیر هسته پیش‌بینی‌ها را انجام دهد بدون نیاز به تبدیل داده‌ها.

### مرحله 1: به یاد آوری توزیع پیشین و درستنمایی

توزیع پیشین برای مقادیر تابع یک فرآیند گوسی است:

$$
f \sim \mathcal{N}(0, K)
$$

که در آن $ K $ ماتریس کوواریانس است که توسط تابع هسته $ k(x, x') = \phi(x)^\top \phi(x') $ داده می‌شود.

درستنمایی به صورت زیر است:

$$
y = f + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma_n^2 I)
$$

که در آن $ y $ خروجی‌های مشاهده‌شده هستند و $ \sigma_n^2 $ واریانس نویز است.

با توجه به ماتریس:

$$
A = \sigma_n^{-2} \Phi \Phi^\top + \Sigma_p^{-1}
$$

که در آن:
- $ \sigma_n^2 $ واریانس نویز است،
- $ \Phi $ ماتریس طراحی (که معمولاً ویژگی‌های داده‌های ورودی را در بر می‌گیرد)،
- $ \Sigma_p^{-1} $ ماتریس کوواریانس پیشین (معمولاً معکوس ماتریس کوواریانس پیشین) است.

برای محاسبه $ A^{-1} $، می‌توانیم از یک هویت ماتریس شناخته‌شده برای معکوس مجموع دو ماتریس استفاده کنیم:

$$
(A + B)^{-1} = A^{-1} - A^{-1} B (I + B A^{-1})^{-1} A^{-1}
$$

با این حال، از آنجایی که $ A $ مجموع دو ماتریس است که در آن $ A = \sigma_n^{-2} \Phi \Phi^\top + \Sigma_p^{-1} $، می‌توانیم فرمول را مستقیماً با توجه به ساختار خاص $ A $ اعمال کنیم.

حالا، با استفاده از **هویت ماتریس وودبری**، که تعمیمی از لmmas معکوس ماتریس است. این هویت زمانی مفید است که ماتریس به صورت $ A = U V^\top + C $ باشد، که در آن $ U $ و $ V $ ماتریس‌ها هستند و $ C $ ماتریس دیگری است.

**هویت وودبری** به صورت زیر است:

$$
(A + U V^\top)^{-1} = A^{-1} - A^{-1} U (I + V^\top A^{-1} U)^{-1} V^\top A^{-1}
$$


**اثبات:**

**هویت ماتریس وودبری** به صورت زیر داده شده است:

$$
(M + U V^\top)^{-1} = M^{-1} - M^{-1} U \left(I + V^\top M^{-1} U\right)^{-1} V^\top M^{-1}
$$

که در آن:
- $ M $ یک ماتریس معکوس‌پذیر $ m \times m $ است،
- $ U $ یک ماتریس $ m \times k $ است،
- $ V $ یک ماتریس $ k \times m $ است،
- $ I $ ماتریس همانی با اندازه $ k \times k $ است.

**شروع با عبارت معکوس**

فرض کنید $ B = M + U V^\top $، و فرض کنید می‌خواهیم $ B^{-1} $ را پیدا کنیم، بنابراین:

$$
B B^{-1} = I
$$

این یعنی:

$$
(M + U V^\top)(B^{-1}) = I
$$

**فرض کردن شکل معکوس**

ما فرض می‌کنیم که $ B^{-1} $ به صورت زیر است:

$$
B^{-1} = M^{-1} - M^{-1} U X V^\top M^{-1}
$$

که در آن $ X $ باید تعیین شود.

**جایگزینی در هویت**

حالا، این فرض را در معادله جایگزین می‌کنیم:

$$
(M + U V^\top)(M^{-1} - M^{-1} U X V^\top M^{-1}) = I
$$

**گسترش محصول**

گسترش ترم‌ها به ما می‌دهد:

$$
M M^{-1} - M M^{-1} U X V^\top M^{-1} + U V^\top M^{-1} - U V^\top M^{-1} U X V^\top M^{-1}
$$

ساده‌سازی:

$$
I - M^{-1} U X V^\top M^{-1} + U V^\top M^{-1} - U V^\top M^{-1} U X V^\top M^{-1}
$$

**حل برای $ X $**

برای تطبیق با هویت $ I $، باید ترم‌های تصحیح را برابر صفر قرار دهیم. این به شرط زیر منجر می‌شود:

$$
X = (I + V^\top M^{-1} U)^{-1}
$$

**عبارت نهایی برای معکوس**

بنابراین، معکوس $ M + U V^\top $ به صورت زیر است:

$$
(M + U V^\top)^{-1} = M^{-1} - M^{-1} U \left(I + V^\top M^{-1} U\right)^{-1} V^\top M^{-1}
$$



در مورد ما، $ A =\Sigma_p^{-1} + \sigma_n^{-2} \Phi \Phi^\top $، بنابراین می‌توانیم هویت را اینجا اعمال کنیم:

$$
A^{-1} = \left( \Sigma_p^{-1} + \sigma_n^{-2} \Phi \Phi^\top \right)^{-1}
$$

با استفاده از هویت وودبری، داریم:

$$
A^{-1} = \Sigma_p - \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p
$$

بنابراین، معکوس $ A $ به صورت زیر است:

$$
A^{-1} = \Sigma_p - \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p
$$

این فرمول برای حل کوواریانس پسین در **رگرسیون خطی بیزی** یا **رگرسیون فرآیند گوسی** حیاتی است و شامل اجزای زیر است:
- $ \Sigma_p $ ماتریس کوواریانس پیشین (معمولاً شناخته شده است)،
- $ \Phi $ ماتریس طراحی (شامل ویژگی‌های داده‌های ورودی شما)،
- $ \sigma_n^2 $ واریانس نویز،
- عضو $ \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} $ بخش کلیدی محاسبات است.



### مرحله 2: توزیع پسین

توزیع پسین $ f_* $ با توجه به $ X $ و $ y $ به صورت زیر است:

$$
f_* | x_*, X, y \sim \mathcal{N}(\mu_*, \Sigma_*)
$$

که در آن میانگین پسین $ \mu_* $ و کوواریانس $ \Sigma_* $ به صورت زیر هستند:

- **میانگین:**

$$
\mu_* = \frac{1}{\sigma_n^2} \phi(x_*)^\top A^{-1} \phi^\top y
$$

- **واریانس:**

$$
\sigma_* = \phi(x_*)^\top A^{-1} \phi(x_*)
$$

### مرحله 3: بیان به صورت هسته

حالا، این موارد را به صورت توابع هسته بیان می‌کنیم.

#### 3.1 میانگین پسین به صورت هسته

میانگین پسین $ \mu_* $ می‌تواند به صورت زیر نوشته شود:

ما با عبارت $ A^{-1} $ شروع می‌کنیم:

$$
A^{-1} = \Sigma_p - \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p
$$

این عبارت را در فرمول میانگین پسین جایگزین می‌کنیم:

$$
\mu_* = \frac{1}{\sigma_n^2} \phi(x_*)^\top A^{-1} \phi^\top y
$$

اثبات:

$$
\sigma_n^{-2} \phi^\top (K+\sigma^2I)
$$

همانطور که $K=\phi^\top\Sigma_p\phi$، داریم:
$$
\sigma_n^{-2} \phi^\top (\phi^\top\Sigma_p\phi+\sigma^2I)
$$

$$
\sigma_n^{-2} \phi^\top \phi \Sigma_p \phi^\top+\phi^\top
$$

سپس $ \Sigma_p \phi^\top $ را از سمت راست فاکتور می‌گیریم.
$$
(\sigma_n^{-2}\phi^\top\phi+\Sigma_p^{-1})\Sigma_p\phi^\top
$$

همانطور که $A=\sigma_n^{-2}\phi^\top\phi+\Sigma_p^{-1}$ داریم:

$$
\sigma_n^{-2}\phi^\top(K+\sigma^2I)=A\Sigma_p\phi^\top
$$

حالا هر دو طرف را در $A^-1$ ضرب می‌کنیم

$$
\sigma_n^{-2}A^{-1}\phi^\top(K+\sigma^2I)=\Sigma_p\phi^\top
$$

حالا هر دو طرف را از سمت راست در $ (K+\sigma^2I) $ ضرب می‌کنیم

$$
\sigma_n^{-2}A^{-1}\phi^\top=\Sigma_p\phi^\top(K+\sigma^2I)^{-1}
$$

سپس همانطور که می‌دانیم $ \mu_*=\sigma_n^{-2}\phi^\top(x_*)A^{-1}\phi^\top y $، داریم:

$$
\mu_*=\phi(x_*)^\top \Sigma_p \phi^\top (K+\sigma^2I)^{-1}y
$$

#### 3.2 کوواریانس پسین به صورت هسته

کوواریانس پسین $ \Sigma_* $ می‌تواند به صورت زیر نوشته شود:
ما با عبارت کوواریانس پسین شروع می‌کنیم:

$$
\text{cov}(f_* | x_*, X, y) = K(x_*, x_*) - K(x_*, X)^\top A^{-1} K(X, x_*)
$$

سپس عبارت $ A^{-1} $ را جایگزین می‌کنیم:

$$
A^{-1} = \Sigma_p - \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p
$$

این به ما می‌دهد:

$$
\text{cov}(f_* | x_*, X, y) = K(x_*, x_*) - K(x_*, X)^\top \left[ \Sigma_p - \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p \right] K(X, x_*)
$$

گسترش ترم‌های داخل براکت‌ها:

$$
\text{cov}(f_* | x_*, X, y) = K(x_*, x_*) - K(x_*, X)^\top \Sigma_p K(X, x_*) + K(x_*, X)^\top \Sigma_p \Phi \left( \sigma_n^2 + \Phi^\top \Sigma_p \Phi \right)^{-1} \Phi^\top \Sigma_p K(X, x_*)
$$

حالا، به صورت هسته بیان می‌کنیم:

$$
\text{cov}(f_* | x_*, X, y) = k(x_*, x_*) - k(x_*, X)^\top K(X, X)^{-1} k(X, x_*) + k(x_*, X)^\top K(X, X)^{-1} \left( \sigma_n^2 + k(X, X) \right)^{-1} K(X, X)^{-1} k(X, x_*)
$$

عبارت نهایی برای کوواریانس پسین به صورت زیر است:

$$
\text{cov}(f_* | x_*, X, y) = k(x_*, x_*) - k(x_*, X)^\top K(X, X)^{-1} k(X, x_*)
$$

### نتیجه نهایی

بنابراین، میانگین و کوواریانس پسین به صورت زیر هستند:

- **میانگین:**

$$
\mu_* = k(x_*, X)^\top K(X, X)^{-1} y
$$

- **کوواریانس:**

$$
\Sigma_* = k(x_*, x_*) - k(x_*, X)^\top K(X, X)^{-1} k(X, x_*)
$$
**فرمول نهایی:**

بنابراین، توزیع پسین نهایی $ f_* $ به صورت زیر است:

$$
f_* | \mathbf{x_*}, \mathbf{X}, \mathbf{y} \sim \mathcal{N}\left( \phi_*^\top \Sigma_p \Phi (K + \sigma_n^2 I)^{-1} \mathbf{y}, \phi_*^\top \Sigma_p \phi_* - \phi_*^\top \Sigma_p \Phi (K + \sigma_n^2 I)^{-1} \Phi^\top \Sigma_p \phi_* \right)
$$

****توضیح اجزای کلیدی:****

- **نگاشت ویژگی** $ \phi_* $ و $ \Phi $: نگاشت ویژگی $ \phi(\cdot) $ داده‌ها را به فضای با ابعاد بالاتر، اغلب بی‌نهایت‌بعدی، تبدیل می‌کند تا غیرخطی بودن داده‌ها را در بر بگیرد. در این معادله، $ \phi_* $