---
layout: persian  # ÛŒØ§ single Ø¨Ø§ Ú©Ù„Ø§Ø³ rtl-layout
classes: wide rtl-layout
dir: rtl
title: "Cross-Entropy"
permalink: /teaching/toolkit/CrossEntropy/
author_profile: true
sidebar:
  nav: "toolkit"
header:
  overlay_image: "/assets/images/background.jpg"
  overlay_filter: 0.3
  overlay_color: "#5e616c"
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
---



# Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù†Ø±ÙˆÙ† Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯ÛŒ Ø¨Ø§ Cross-Entropy

Ù…Ø¯Ù„ Ù†Ø±ÙˆÙ†:

$$
\hat y = g(z) = \sigma(z) = \sigma(w^T x + b)
$$

Ú©Ù‡ Ø¯Ø± Ø¢Ù†:

- $x$ Ø¨Ø±Ø¯Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
- $w$ ÙˆØ²Ù†â€ŒÙ‡Ø§
- $b$ Ø¨Ø§ÛŒØ§Ø³
- $z = w^T x + b$
- $\sigma(z) = \frac{1}{1+e^{-z}}$

Ù…Ø³Ø¦Ù„Ù‡: Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯ÙˆØ¯ÙˆÛŒÛŒ

---

# ØªØ§Ø¨Ø¹ Ø¶Ø±Ø± Cross-Entropy Ø¯ÙˆØ¯ÙˆÛŒÛŒ

Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ $y \in \{0,1\}$:

$$
L = -\big[y \log(\hat y) + (1-y)\log(1-\hat y)\big]
$$

---

# Ù‡Ø¯Ù ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ

$$
\min_{w,b} L
$$

Ø¨Ø§ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ú©Ø§Ù‡Ø´ÛŒ:

$$
w \leftarrow w - \eta \frac{\partial L}{\partial w}
$$

$$
b \leftarrow b - \eta \frac{\partial L}{\partial b}
$$

---

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† â€” Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ù…Ø±Ø­Ù„Ù‡

## Ù…Ø´ØªÙ‚ loss Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒ

$$
\frac{\partial L}{\partial \hat y}
=
-\left(\frac{y}{\hat y} - \frac{1-y}{1-\hat y}\right)
$$

---

## Ù…Ø´ØªÙ‚ Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯

$$
\frac{d\sigma}{dz} = \sigma(z)(1-\sigma(z)) = \hat y(1-\hat y)
$$

---

## Ù†ØªÛŒØ¬Ù‡ Ù…Ù‡Ù… (Ø³Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø²Ø±Ú¯)

Ø¨Ø§ Ù‚Ø§Ø¹Ø¯Ù‡ Ø²Ù†Ø¬ÛŒØ±Ù‡:

$$
\frac{\partial L}{\partial z}
=
\hat y - y
$$

Ø§ÛŒÙ† Ø³Ø§Ø¯Ù‡ Ø´Ø¯Ù† Ø¯Ù„ÛŒÙ„ Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Cross-Entropy Ø¨Ø§ Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯ Ø§Ø³Øª.

---

# Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ù†Ø³Ø¨Øª Ø¨Ù‡ ÙˆØ²Ù†â€ŒÙ‡Ø§

Ú†ÙˆÙ†:

$$
z = w^T x + b
$$

Ø¯Ø§Ø±ÛŒÙ…:

$$
\frac{\partial z}{\partial w} = x
$$

Ù¾Ø³:

$$
\frac{\partial L}{\partial w}
=
(\hat y - y) x
$$

---

# Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¨Ø§ÛŒØ§Ø³

$$
\frac{\partial L}{\partial b}
=
\hat y - y
$$

---

# Ù‚Ø§Ù†ÙˆÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§

## ÙˆØ²Ù†â€ŒÙ‡Ø§:

$$
w \leftarrow w - \eta (\hat y - y)x
$$

## Ø¨Ø§ÛŒØ§Ø³:

$$
b \leftarrow b - \eta (\hat y - y)
$$

---

# ØªÙØ³ÛŒØ± Ø´Ù‡ÙˆØ¯ÛŒ

ØªØ±Ù… Ø®Ø·Ø§:

$$
(\hat y - y)
$$

- Ø§Ú¯Ø± $\hat y > y$ â†’ Ø¨ÛŒØ´â€ŒØ¨Ø±Ø¢ÙˆØ±Ø¯ â†’ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ú©Ø§Ù‡Ø´
- Ø§Ú¯Ø± $\hat y < y$ â†’ Ú©Ù…â€ŒØ¨Ø±Ø¢ÙˆØ±Ø¯ â†’ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø§ÙØ²Ø§ÛŒØ´

---

# ÙØ±Ù… Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Batch

Ø§Ú¯Ø± $X$ Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø´Ø¯:

$$
\nabla_w L = X^T(\hat y - y)
$$


# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Cross-Entropy Ø¯Ø± Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¹Ù…ÛŒÙ‚

Ø¯Ø± Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¹Ù…ÛŒÙ‚ØŒ Cross-Entropy Ù¾Ø±Ú©Ø§Ø±Ø¨Ø±Ø¯ØªØ±ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¶Ø±Ø± Ø¨Ø±Ø§ÛŒ **Ù…Ø³Ø§Ø¦Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ** Ø§Ø³Øª.  
Ø§ÛŒØ¯Ù‡ Ù‡Ù…Ø§Ù† Ø§Ø³Øª Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± Ù†Ø±ÙˆÙ† Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯ÛŒ Ø¯ÛŒØ¯ÛŒØ¯ â€” ÙÙ‚Ø· Ø§Ú©Ù†ÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ Ø§Ø² Ú†Ù†Ø¯ÛŒÙ† Ù„Ø§ÛŒÙ‡ Ø¹Ø¨ÙˆØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø¨Ø§ **Backpropagation** Ø¨Ù‡ Ù‡Ù…Ù‡ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

---

# Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ Ø´Ø¨Ú©Ù‡

$$
x â†’ Layers â†’ z^{(L)} â†’ g(z^{(L)}) â†’ \hat y
$$

- $z^{(L)}$ Ø®Ø±ÙˆØ¬ÛŒ Ø®Ø·ÛŒ Ù„Ø§ÛŒÙ‡ Ø¢Ø®Ø±
- $g$ ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
- $\hat y$ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§ØªÛŒ

---

# Ø§Ù†ØªØ®Ø§Ø¨ ØªØ§Ø¨Ø¹ Ø®Ø±ÙˆØ¬ÛŒ + Cross-Entropy

## âœ… Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯ÙˆØ¯ÙˆÛŒÛŒ

Ø®Ø±ÙˆØ¬ÛŒ: **Sigmoid**

$$
\hat y = \sigma(z)
$$

Cross-Entropy Ø¯ÙˆØ¯ÙˆÛŒÛŒ:

$$
L = - [ y \log(\hat y) + (1-y)\log(1-\hat y) ]
$$

---

## âœ… Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡

Ø®Ø±ÙˆØ¬ÛŒ: **Softmax**

$$
\hat y_i = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

Cross-Entropy Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡:

$$
L = - \sum_i y_i \log(\hat y_i)
$$

(Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ one-hot Ù‡Ø³ØªÙ†Ø¯)

---

# Ø®Ø§ØµÛŒØª Ø·Ù„Ø§ÛŒÛŒ Softmax + Cross-Entropy

Ø§Ú¯Ø± Softmax Ùˆ Cross-Entropy Ø¨Ø§ Ù‡Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯:

## Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø¨Ø³ÛŒØ§Ø± Ø³Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯:

$$
\frac{\partial L}{\partial z_i} = \hat y_i - y_i
$$

ðŸ‘‰ Ø§ÛŒÙ† Ù‡Ù…Ø§Ù† Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯ Ù‡Ù… Ø¯ÛŒØ¯ÛŒÙ….

---

# Ø±ÙˆÙ†Ø¯ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø± Ø´Ø¨Ú©Ù‡ Ø¹Ù…ÛŒÙ‚

# Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§

## PyTorch

```python
loss = torch.nn.CrossEntropyLoss()
```
TensorFlow / Keras
loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

Ø®Ù„Ø§ØµÙ‡ ØªØµÙˆÛŒØ±ÛŒ Ø¬Ø±ÛŒØ§Ù† Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†
Softmax + CrossEntropy
        â†“
delta = y_hat âˆ’ y
        â†“
Backprop Ø¯Ø± Ú©Ù„ Ø´Ø¨Ú©Ù‡
        â†“
Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù‡Ù…Ù‡ ÙˆØ²Ù†â€ŒÙ‡Ø§



