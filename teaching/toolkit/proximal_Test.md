```yaml
---
layout: persian
classes: wide rtl-layout
dir: rtl
title: "الگوریتم‌های پروگزیمال"
permalink: /teaching/toolkit/ProximalMapping111/
author_profile: true
sidebar:
  nav: "toolkit"
header:
  overlay_image: "/assets/images/background.jpg"
  overlay_filter: 0.3
  overlay_color: "#5e616c"
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
---
```

# الگوریتم‌های پروگزیمال

## مقدمه و کلیات

### جایگاه الگوریتم‌های پروگزیمال
الگوریتم‌های پروگزیمال یک دسته قدرتمند از روش‌های بهینه‌سازی هستند که در سطح انتزاع بالاتری نسبت به الگوریتم‌های کلاسیک مانند **روش نیوتن** قرار می‌گیرند. در حالی که روش نیوتن یک ابزار استاندارد برای حل مسائل بهینه‌سازی **نرم** (smooth) بدون قید با اندازه متوسط است، الگوریتم‌های پروگزیمال را می‌توان ابزاری مشابه برای معادلات **ناهموار**، **مقید**، **بزرگ‌مقیاس** و نسخه‌های توزیع‌شده دانست.

### کاربردهای اصلی
- مواجهه با مجموعه داده‌های بزرگ یا با ابعاد بالا
- حل مسائل بهینه‌سازی با توابع غیرمشتق‌پذیر
- مسائل با قیود مختلف
- مسائل توزیع‌شده در یادگیری ماشین

### فلسفه اصلی
**پروگزیمال یک روش ریاضی است برای این‌که وقتی یک مسئلهٔ بهینه‌سازی «سخت» یا «غیرقابل مشتق» داریم، آن را به یک مسئلهٔ ساده‌تر و قابل‌حل تبدیل کنیم.**

---

## مفاهیم پایه

### ۱. نگاشت پروگزیمال

#### شهود پروگزیمال
فرض کنید می‌خواهید یک تابع را کمینه کنید، اما:
- یا گوشه‌دار است (مثل قدرمطلق)
- یا مشتق‌پذیر نیست
- یا قیدهای دردسرساز دارد

در این حالت، **Proximal Mapping** می‌گوید: به‌جای این‌که مستقیم سراغ تابع سخت برویم، یک نقطهٔ فعلی را می‌گیریم و نزدیک‌ترین نقطه‌ای را پیدا می‌کنیم که هم به آن نقطه نزدیک باشد و هم مقدار تابع را کوچک کند.

#### تعریف ریاضی
برای یک تابع \( f(x) \):

\[
\text{prox}_{f}(v) = \arg\min_x \left( f(x) + \frac{1}{2}\|x - v\|^2 \right)
\]

**توضیح اجزا:**
- `v` : نقطه‌ی فعلی  
- \( f(x) \) : تابعی که کمینه می‌شود  
- جمله‌ی دوم: جریمه برای دور شدن زیاد از `v`

#### مثال ساده: تابع قدرمطلق
اگر:
\[
f(x) = |x|
\]
آنگاه پروگزیمال مپ برابر است با **Soft Thresholding**:

\[
\text{prox}_{\lambda |x|}(v) =
\begin{cases}
v - \lambda & v > \lambda \\
0 & |v| \le \lambda \\
v + \lambda & v < -\lambda
\end{cases}
\]

**تفسیر:**
- مقادیر کوچک → صفر می‌شوند  
- مقادیر بزرگ → به سمت صفر کشیده می‌شوند

### ۲. فضای هیلبرت

#### تعریف پایه
**فضای هیلبرت** یک فضای برداری است که دو ویژگی کلیدی دارد:
1. **دارای ضرب داخلی** (inner product)
2. **کامل** (complete) است نسبت به نرمی که از آن ضرب داخلی مشتق می‌شود

#### ویژگی‌های مهم برای بهینه‌سازی
1. **وجود تصویر**: برای هر زیرمجموعه‌ی بسته و محدب \( C \) و هر نقطه‌ی \( x \)، یک نقطه‌ی نزدیک‌ترین منحصربه‌فرد در \( C \) وجود دارد.
2. **اکیداً محدب بودن نرم**: تابع \( x \mapsto \| x \|^2 \) در فضای هیلبرت اکیداً محدب است.
3. **قضیۀ نمایندگی رایز**: هر تابع خطی پیوسته روی فضای هیلبرت را می‌توان به صورت ضرب داخلی با یک بردار ثابت نوشت.

---

## الگوریتم نقطه پروگزیمال

### فرمول‌بندی الگوریتم
روش نقطۀ پروکسیمال بر اساس **منظم‌سازی** مسئلۀ اصلی بنا شده است. به این معنی که به جای حل مستقیم مسئلۀ کمینه‌سازی برای تابع محدب \( f \)، توجه خود را معطوف می‌کنیم به دنباله‌ای از مسائل کمینه‌سازی که حاصلِ **اغتشاش** تابع هدف اصلی با یک تابع نرمی از فضای \( H \) هستند.

فرض کنید \( x_0 \in H \) نقطه شروع باشد. اگر \( x_{n-1} \) داده شده باشد و \( \lambda > 0 \)، آن‌گاه \( x_n \) را به عنوان جواب مسئلۀ کمینه‌سازی زیر تعریف می‌کنیم:

\[
\min_{x \in H} \left\{ f(x) + \frac{1}{2\lambda} \| x - x_{n-1} \|^2 \right\} \tag{1.3}
\]

### همگرایی الگوریتم (نتیجۀ مارتینه)
**مارتینه** ثابت کرد که اگر:
\[
\arg\min f \neq \emptyset
\]
یعنی تابع \( f \) حداقل یک نقطۀ کمینه داشته باشد، آنگاه دنبالۀ \( \{x_n\} \) تولیدشده توسط رابطه‌ی (۱.۳) **همگرای ضعیف** به یک نقطۀ کمینه‌ی تابع \( f \) خواهد بود.

### بازنویسی با نگاشت پروگزیمال
با استفاده از نگاشت پروگزیمال، الگوریتم مارتینه به صورت فشرده زیر قابل بازنویسی است:

\[
x_n = \text{Prox}_{\lambda f} (x_{n-1}).
\]

به این ترتیب، هر گام الگوریتم تنها یک **ارزیابی پروکسیمال** است که نقطۀ قبلی را به نقطۀ بعدی نگاشت می‌کند.

---

## Proximal Gradient Method

### صورت کلی مسأله
می‌خواهیم مسأله‌ای از این جنس را حل کنیم:

\[
\min_{x \in H} \left\{ g(x) + h(x)\right\}
\]

که در آن:
- \( g \) تابعی **صاف و مشتق‌پذیر** است (مثلاً least squares)
- \( h \) تابعی است که ممکن است **غیرصاف** باشد (مثل \( \|x\|_1 \))، ولی «prox» آن قابل محاسبه است.

### فرمول به‌روزرسانی
الگوریتم می‌گوید:

\[
x^+ = \text{prox}_{th} ( x - t \nabla g(x) )
\]

### تفسیر الگوریتم
1. **اول یک قدم گرادیانی روی \( g \) می‌رویم**
2. **بعد نتیجه را از فیلتر prox مربوط به \( h \) رد می‌کنیم**

### مصادیق خاص
1. **وقتی \( h(x) = 0 \)**:
   \[
   \text{prox}_{t0}(u) = u
   \]
   پس الگوریتم می‌شود همان گرادیان‌دسنت معمولی.

2. **وقتی \( h(x) = \|x\|_1 \)** (LASSO / sparse recovery):
   prox این تابع، **soft-thresholding** است. برای هر مؤلفه:
   \[
   \text{prox}_{th}(u)_i =
   \begin{cases}
   u_i - t & \text{if } u_i > t \\
   0 & \text{if } |u_i| \le t \\
   u_i + t & \text{if } u_i < -t
   \end{cases}
   \]

---

## مثال کاربردی: رگرسیون با جریمه L1

### صورت مسأله
داده‌ها:
\[
D=\{(x_i,y_i)\}_{i=1}^n
\]

مدل خطی:
\[
y \approx ax + b
\]

تابع هدف:
\[
\min_{a,b}\; g(a,b)+h(a,b)
\]
که در آن:
\[
g(a,b)=\frac12\sum_{i=1}^n (ax_i+b-y_i)^2
\]
\[
h(a,b)=\lambda |a|
\]

### الگوریتم
**گام گرادیانی:**
\[
a^{temp}=a^{(k)}-t\sum_{i=1}^n (ax_i+b-y_i)x_i
\]
\[
b^{(k+1)}=b^{(k)}-t\sum_{i=1}^n (ax_i+b-y_i)
\]

**گام Proximal (فقط روی \(a\)):**
\[
a^{(k+1)}=\text{sign}(a^{temp}) \max(|a^{temp}|-t\lambda,0)
\]

### تفسیر نتایج
- \(b\): آپدیت معمولی گرادیانی  
- \(a\): حرکت گرادیانی + عبور از **soft-thresholding**  
- مقادیر کوچک → صفر  
- مقادیر بزرگ → کوچک‌تر اما باقی می‌مانند  

**نتیجه:** مدل sparse و پایدارتر

---

## پیوست: محاسبه Proximal Mapping برای تابع قدر مطلق

### تعریف مسئله
طبق تعریف:
\[
\begin{aligned}
\text{prox}_{\lambda |x|}(v)
&= \arg\min_x \left( \lambda |x| + \frac{1}{2}(x - v)^2 \right)
\end{aligned}
\]

### تحلیل سه حالت
1. **حالت \( x > 0 \)**:
   \[
   x = v - \lambda \quad \text{به شرط } v > \lambda
   \]

2. **حالت \( x < 0 \)**:
   \[
   x = v + \lambda \quad \text{به شرط } v < -\lambda
   \]

3. **حالت \( x = 0 \)**:
   \[
   x = 0 \quad \text{به شرط } |v| \le \lambda
   \]

### نتیجه نهایی
\[
\text{prox}_{\lambda |x|}(v) =
\begin{cases}
v - \lambda & v > \lambda \\
0 & |v| \le \lambda \\
v + \lambda & v < -\lambda
\end{cases}
\]

### فرم فشرده
\[
\begin{aligned}
\text{prox}_{\lambda |x|}(v) &= \mathrm{sign}(v) \cdot \max\bigl(|v| - \lambda, 0\bigr)
\end{aligned}
\]
---

## جمع‌بندی و کاربردها

### مزایای الگوریتم‌های پروگزیمال
- مدیریت توابع غیرمشتق‌پذیر
- پیاده‌سازی الگوریتم‌های پیشرفته:
  - **Proximal Gradient**
  - **ISTA / FISTA**
  - **ADMM**
- پایه‌ی الگوریتم‌های مدرن یادگیری ماشین

### حوزه‌های کاربردی
1. **بازیابی پراکنده** (Sparse Recovery)
2. **یادگیری عمیق منظم‌شده**
3. **پردازش سیگنال و تصویر**
4. **آمار روباست**

### نکته پایانی
الگوریتم‌های پروگزیمال نه جادو هستند و نه ترفند؛ فقط استفاده‌ای دقیق از هندسه‌ی محدب‌ها و ساختارهای فضایی مناسب برای حل مسائل بهینه‌سازی مدرن می‌باشند.